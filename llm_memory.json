{
  "files": [
    {
      "filename": "hippocampus.rb",
      "content": "require_relative 'store'\nrequire_relative 'stores/redis_store'\n\nrequire_relative 'embedding'\nrequire_relative 'embeddings/openai'\n\nmodule LlmMemory\n  class Hippocampus\n    def initialize(\n      embedding_name: :openai,\n      chunk_size: 1024,\n      chunk_overlap: 50,\n      store: :redis,\n      index_name: 'llm_memory'\n    )\n      LlmMemory.configure\n\n      embedding_class = EmbeddingManager.embeddings[embedding_name]\n      raise \"Embedding '#{embedding_name}' not found.\" unless embedding_class\n\n      @embedding_instance = embedding_class.new\n\n      store_class = StoreManager.stores[store]\n      raise \"Store '#{store}' not found.\" unless store_class\n\n      @store = store_class.new(index_name: index_name)\n\n      # char count, not word count\n      @chunk_size = chunk_size\n      @chunk_overlap = chunk_overlap\n    end\n\n    # validate the document format\n    def validate_documents(documents)\n      is_valid = documents.all? do |hash|\n        hash.is_a?(Hash) &&\n          hash.key?(:content) && hash[:content].is_a?(String) &&\n          hash.key?(:metadata) && hash[:metadata].is_a?(Hash)\n      end\n      return if is_valid\n\n      raise 'Your documents need to have an array of hashes (content: string and metadata: hash)'\n    end\n\n    def memorize(docs)\n      validate_documents(docs)\n      docs = make_chunks(docs)\n      docs = add_vectors(docs)\n      @store.create_index unless @store.index_exists?\n      @store.add(data: docs)\n    end\n\n    def query(query_str, limit: 3)\n      vector = @embedding_instance.embed_document(query_str)\n      @store.search(query: vector, k: limit)\n    end\n\n    def forget_all\n      @store.drop_index if @store.index_exists?\n    end\n\n    def forget(key)\n      @store.delete(key)\n    end\n\n    def list(*args)\n      @store.list(*args)\n    end\n\n    def get(key)\n      @store.get(key)\n    end\n\n    def add_vectors(docs)\n      # embed documents and add vector\n      result = []\n      docs.each do |doc|\n        content = doc[:content]\n        metadata = doc[:metadata]\n        vector = @embedding_instance.embed_document(content)\n        result.push({\n                      content: content,\n                      metadata: metadata,\n                      vector: vector\n                    })\n      end\n      result\n    end\n\n    def make_chunks(docs)\n      result = []\n      docs.each do |item|\n        content = item[:content]\n        metadata = item[:metadata]\n        if content.length > @chunk_size\n          start_index = 0\n          while start_index < content.length\n            end_index = [start_index + @chunk_size, content.length].min\n            chunk = content[start_index...end_index]\n            result << { content: chunk, metadata: metadata }\n            break if end_index == content.length\n\n            start_index += @chunk_size - @chunk_overlap\n          end\n        else\n          result << { content: content, metadata: metadata }\n        end\n      end\n      result\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "file_loader.rb",
      "content": "require 'find'\nrequire_relative '../loader'\n\nmodule LlmMemory\n  class FileLoader\n    include Loader\n\n    register_loader :file\n\n    def load(directory_path)\n      files_array = []\n      Find.find(directory_path) do |file_path|\n        next if File.directory?(file_path)\n\n        file_name = File.basename(file_path)\n        file_content = File.read(file_path)\n        ctime = File.ctime(file_path)\n\n        files_array << {\n          content: file_content,\n          metadata: {\n            file_name: file_name,\n            timestamp: ctime.strftime('%Y%m%d%H%M%S') # YYMMDDHHmmss\n          }\n        }\n      end\n\n      files_array\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/loaders/"
    },
    {
      "filename": "loader.rb",
      "content": "# lib/lilyama_index/loader.rb\nmodule LlmMemory\n  module Loader\n    def self.included(base)\n      base.extend(ClassMethods)\n    end\n\n    module ClassMethods\n      def register_loader(name)\n        LlmMemory::LoaderManager.register_loader(name, self)\n      end\n    end\n\n    def load\n      raise NotImplementedError, \"Each loader must implement the 'load' method.\"\n    end\n  end\n\n  class LoaderManager\n    @loaders = {}\n\n    def self.register_loader(name, klass)\n      @loaders[name] = klass\n    end\n\n    class << self\n      attr_reader :loaders\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "openai.rb",
      "content": "require_relative '../embedding'\nrequire_relative '../llms/openai'\n\nmodule LlmMemory\n  module Embeddings\n    class Openai\n      include LlmMemory::Embedding\n      include Llms::Openai\n\n      register_embedding :openai\n\n      def embed_documents(texts, model: 'text-embedding-ada-002')\n        embedding_list = []\n        texts.each do |txt|\n          res = client.embeddings(\n            parameters: {\n              model: model,\n              input: txt\n            }\n          )\n          embedding_list.push(res['data'][0]['embedding'])\n        end\n        embedding_list\n      end\n\n      def embed_document(text, model: 'text-embedding-ada-002')\n        res = client.embeddings(\n          parameters: {\n            model: model,\n            input: text\n          }\n        )\n        res['data'][0]['embedding']\n      end\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/embeddings/"
    },
    {
      "filename": "broca.rb",
      "content": "require 'erb'\nrequire 'tokenizers'\n\nmodule LlmMemory\n  class Broca\n    include Llms::Openai\n    attr_accessor :messages\n\n    def initialize(\n      prompt:,\n      model: 'gpt-3.5-turbo',\n      temperature: 0.7,\n      max_token: 4096\n    )\n      LlmMemory.configure\n      @prompt = prompt\n      @model = model\n      @messages = []\n      @temperature = temperature\n      @max_token = max_token\n    end\n\n    def respond(args)\n      final_prompt = generate_prompt(args)\n      @messages.push({ role: 'user', content: final_prompt })\n      adjust_token_count\n      begin\n        response = client.chat(\n          parameters: {\n            model: @model,\n            messages: @messages,\n            temperature: @temperature\n          }\n        )\n        LlmMemory.logger.debug(response)\n        response_content = response.dig('choices', 0, 'message', 'content')\n        @messages.push({ role: 'system', content: response_content }) unless response_content.nil?\n        response_content\n      rescue StandardError => e\n        LlmMemory.logger.info(e.inspect)\n        # @messages = []\n        nil\n      end\n    end\n\n    def respond_with_schema(context: {}, schema: {})\n      response_content = respond(context)\n      begin\n        response = client.chat(\n          parameters: {\n            model: 'gpt-3.5-turbo-0613', # as of July 3, 2023\n            messages: [\n              {\n                role: 'user',\n                content: response_content\n              }\n            ],\n            functions: [\n              {\n                name: 'broca',\n                description: 'Formating the content with the specified schema',\n                parameters: schema\n              }\n            ]\n          }\n        )\n        LlmMemory.logger.debug(response)\n        message = response.dig('choices', 0, 'message')\n        if message['role'] == 'assistant' && message['function_call']\n          function_name = message.dig('function_call', 'name')\n          args =\n            JSON.parse(\n              message.dig('function_call', 'arguments'),\n              { symbolize_names: true }\n            )\n          args if function_name == 'broca'\n        end\n      rescue StandardError => e\n        LlmMemory.logger.info(e.inspect)\n        nil\n      end\n    end\n\n    def generate_prompt(args)\n      erb = ERB.new(@prompt)\n      erb.result_with_hash(args)\n    end\n\n    def adjust_token_count\n      count = 0\n      new_messages = []\n      @messages.reverse_each do |message|\n        encoded = tokenizer.encode(message[:content], add_special_tokens: true)\n        token_count = encoded.tokens.length\n        count += token_count\n        break unless count <= @max_token\n\n        new_messages.push(message)\n      end\n      @messages = new_messages.reverse\n    end\n\n    def tokenizer\n      @tokenizer ||= Tokenizers.from_pretrained('gpt2')\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "wernicke.rb",
      "content": "# loader\nrequire_relative 'loader'\nrequire_relative 'loaders/file_loader'\n\nmodule LlmMemory\n  class Wernicke\n    def self.load(loader_name, *args)\n      loader_class = LoaderManager.loaders[loader_name]\n      raise \"Loader '#{loader_name}' not found.\" unless loader_class\n\n      loader_instance = loader_class.new\n      loader_instance.load(*args)\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "openai.rb",
      "content": "require 'openai'\nrequire 'llm_memory'\n\nmodule LlmMemory\n  module Llms\n    module Openai\n      def client\n        @client ||= OpenAI::Client.new(\n          access_token: LlmMemory.configuration.openai_access_token\n        )\n      end\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/llms/"
    },
    {
      "filename": "embedding.rb",
      "content": "# lib/lilyama_index/Embedding.rb\nmodule LlmMemory\n  module Embedding\n    def self.included(base)\n      base.extend(ClassMethods)\n    end\n\n    module ClassMethods\n      def register_embedding(name)\n        LlmMemory::EmbeddingManager.register_embedding(name, self)\n      end\n    end\n\n    def embed_document(text)\n      raise NotImplementedError, \"Each Embedding must implement the 'embed_document' method.\"\n    end\n  end\n\n  class EmbeddingManager\n    @embeddings = {}\n\n    def self.register_embedding(name, klass)\n      @embeddings[name] = klass\n    end\n\n    class << self\n      attr_reader :embeddings\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "store.rb",
      "content": "# lib/lilyama_index/store.rb\nmodule LlmMemory\n  module Store\n    def self.included(base)\n      base.extend(ClassMethods)\n    end\n\n    module ClassMethods\n      def register_store(name)\n        LlmMemory::StoreManager.register_store(name, self)\n      end\n    end\n\n    def create_index\n      raise NotImplementedError, \"Each store must implement the 'create_index' method.\"\n    end\n\n    def index_exists?\n      raise NotImplementedError, \"Each store must implement the 'index_exists?' method.\"\n    end\n\n    def drop_index\n      raise NotImplementedError, \"Each store must implement the 'drop_index' method.\"\n    end\n\n    def add\n      raise NotImplementedError, \"Each store must implement the 'add' method.\"\n    end\n\n    def list\n      raise NotImplementedError, \"Each store must implement the 'list' method.\"\n    end\n\n    def delete\n      raise NotImplementedError, \"Each store must implement the 'delete' method.\"\n    end\n\n    def search\n      raise NotImplementedError, \"Each store must implement the 'search' method.\"\n    end\n  end\n\n  class StoreManager\n    @stores = {}\n\n    def self.register_store(name, klass)\n      @stores[name] = klass\n    end\n\n    class << self\n      attr_reader :stores\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "version.rb",
      "content": "# frozen_string_literal: true\n\nmodule LlmMemory\n  VERSION = '0.1.14'\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "configuration.rb",
      "content": "module LlmMemory\n  class Configuration\n    attr_accessor :openai_access_token, :openai_organization_id, :redis_url\n\n    def initialize\n      @openai_access_token = ENV['OPENAI_ACCESS_TOKEN']\n      @openai_organization_id = nil\n      @redis_url = ENV['REDISCLOUD_URL'] || 'redis://localhost:6379'\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/"
    },
    {
      "filename": "redis_store.rb",
      "content": "require 'redis'\nrequire_relative '../store'\nrequire 'json'\n\nmodule LlmMemory\n  class RedisStore\n    include Store\n\n    register_store :redis\n\n    def initialize(\n      index_name: 'llm_memory',\n      content_key: 'content',\n      vector_key: 'vector',\n      metadata_key: 'metadata'\n    )\n      @index_name = index_name\n      @content_key = content_key\n      @vector_key = vector_key\n      @metadata_key = metadata_key\n      @client = Redis.new(url: LlmMemory.configuration.redis_url)\n    end\n\n    def info\n      @client.call(['INFO'])\n    end\n\n    def load_data(file_path)\n      CSV.read(file_path)\n    end\n\n    def list_indexes\n      @client.call('FT._LIST')\n    end\n\n    def index_exists?\n      begin\n        @client.call(['FT.INFO', @index_name])\n      rescue StandardError\n        return false\n      end\n      true\n    end\n\n    def drop_index\n      # DD deletes all document hashes\n      @client.call(['FT.DROPINDEX', @index_name, 'DD'])\n    end\n\n    # dimention: 1536 for ada-002\n    def create_index(dim: 1536, distance_metric: 'COSINE')\n      # LangChain index\n      # schema = (\n      #   TextField(name=content_key),\n      #   TextField(name=metadata_key),\n      #   VectorField(\n      #       vector_key,\n      #       \"FLAT\",\n      #       {\n      #           \"TYPE\": \"FLOAT32\",\n      #           \"DIM\": dim,\n      #           \"DISTANCE_METRIC\": distance_metric,\n      #       },\n      #   ),\n      # )\n      command = [\n        'FT.CREATE', @index_name, 'ON', 'HASH',\n        'PREFIX', '1', \"#{@index_name}:\",\n        'SCHEMA',\n        @content_key, 'TEXT',\n        @metadata_key, 'TEXT',\n        @vector_key, 'VECTOR', 'FLAT', 6, 'TYPE', 'FLOAT32', 'DIM', dim, 'DISTANCE_METRIC', distance_metric\n      ]\n      @client.call(command)\n    end\n\n    # data = [{ content: \"\", content_vector: [], metadata: {} }]\n    def add(data: [])\n      result = {}\n      @client.pipelined do |pipeline|\n        data.each_with_index do |d, i|\n          key = @index_name # index_name:create_time:metadata_timestamp:uuid\n          timestamp = d.dig(:metadata, :timestamp)\n          key += \":#{Time.now.strftime('%Y%m%d%H%M%S')}\"\n          key += \":#{timestamp}\"\n          key += \":#{SecureRandom.hex(8)}\"\n\n          meta_json = d[:metadata].nil? ? '' : d[:metadata].to_json # serialize\n          vector_value = d[:vector].map(&:to_f).pack('f*')\n          pipeline.hset(\n            key,\n            {\n              @content_key => d[:content],\n              @vector_key => vector_value,\n              @metadata_key => meta_json\n            }\n          )\n          result[key] = d[:content]\n        end\n      end\n      result\n    # data.each_with_index do |d, i|\n    #   key = \"#{@index_name}:#{i}\"\n    #   vector_value = d[:content_vector].map(&:to_f).pack(\"f*\")\n    #   pp vector_value\n    #   @client.hset(\n    #     key,\n    #     {\n    #       @content_key => d[:content],\n    #       @vector_key => vector_value,\n    #       @metadata_key => \"\"\n    #     }\n    #   )\n    # end\n    # rescue Redis::Pipeline::Error => e\n    #   # Handle the error if there is any issue with the pipeline execution\n    #   puts \"Pipeline Error: #{e.message}\"\n    # rescue Redis::BaseConnectionError => e\n    #   # Handle connection errors\n    #   puts \"Connection Error: #{e.message}\"\n    rescue StandardError => e\n      # Handle any other errors\n      puts \"Unexpected Error: #{e.message}\"\n    end\n\n    def delete(key)\n      @client.del(key) if @client.exists?(key)\n    end\n\n    def delete_all\n      list.keys.each do |key|\n        delete(key)\n      end\n    end\n\n    def get(key)\n      @client.hgetall(key)\n    end\n\n    def list(*args)\n      pattern = \"#{@index_name}:#{args.first || '*'}\"\n      @client.keys(pattern)\n    end\n\n    def update\n    end\n\n    def search(query: [], k: 3)\n      packed_query = query.map(&:to_f).pack('f*')\n      command = [\n        'FT.SEARCH',\n        @index_name,\n        \"*=>[KNN #{k} @vector $blob AS vector_score]\",\n        'PARAMS',\n        2,\n        'blob',\n        packed_query,\n        'SORTBY',\n        'vector_score',\n        'ASC',\n        'LIMIT',\n        0,\n        k,\n        'RETURN',\n        3,\n        'vector_score',\n        @content_key,\n        @metadata_key,\n        'DIALECT',\n        2\n      ]\n      response_list = @client.call(command)\n      response_list.shift # the first one is the size\n      # now [redis_key1, [],,, ]\n      result = response_list.each_slice(2).to_h.values.map do |v|\n        v.each_slice(2).to_h.transform_keys(&:to_sym)\n      end\n      result.each do |item|\n        hash = JSON.parse(item[:metadata])\n        item[:metadata] = hash.transform_keys(&:to_sym)\n      end\n      result\n    end\n  end\nend\n",
      "path": "/lib/llm_memory/stores/"
    },
    {
      "filename": "llm_memory.rb",
      "content": "# frozen_string_literal: true\n\nrequire 'logger'\n# config\nrequire_relative 'llm_memory/configuration'\nrequire_relative 'llm_memory/hippocampus'\nrequire_relative 'llm_memory/broca'\nrequire_relative 'llm_memory/wernicke'\nrequire_relative 'llm_memory/version'\n\nmodule LlmMemory\n  class Error < StandardError; end\n\n  class << self\n    attr_accessor :configuration, :log_level\n\n    def logger\n      @logger ||= Logger.new($stdout).tap do |logger|\n        logger.level = log_level || Logger::INFO\n      end\n    end\n  end\n\n  def self.configure\n    self.configuration ||= Configuration.new\n    yield(configuration) if block_given?\n  end\nend\n",
      "path": "/lib/"
    },
    {
      "filename": "broca_spec.rb",
      "content": "require \"spec_helper\"\nrequire \"llm_memory/broca\"\n\nRSpec.describe LlmMemory::Broca do\n  template = <<~TEMPLATE\n    Context information is below.\n    ---------------------\n    <% related_docs.each do |doc| %>\n    <%= doc[:content] %>\n    \n    <% end %>\n    ---------------------\n    Given the context information and not prior knowledge,\n    answer the question: <%= query_str %>\n  TEMPLATE\n\n  describe \"new\" do\n    it \"instantiates a new Broca object\" do\n      broca = LlmMemory::Broca.new(prompt: \"erb_template\")\n      expect(broca).to be_a(LlmMemory::Broca)\n    end\n  end\n\n  describe \"generate_prompt\" do\n    it \"creates prompt with erb\" do\n      related_docs = [{content: \"foo\"}, {content: \"bar\"}]\n      broca = LlmMemory::Broca.new(prompt: template)\n      prompt = broca.generate_prompt(related_docs: related_docs, query_str: \"how are you?\")\n      expect(prompt).to include(\"foo\")\n      expect(prompt).to include(\"how are you?\")\n    end\n  end\n\n  describe \"adjust_token_count\" do\n    it \"should count token\", :vcr do\n      broca = LlmMemory::Broca.new(prompt: \"I\", max_token: 4)\n      broca.messages = [{content: \"foo bar\"}, {content: \"this is my pen\"}]\n      broca.adjust_token_count\n      expect(broca.messages).to eq([{content: \"this is my pen\"}])\n    end\n  end\n\n  describe \"generate_prompt\" do\n    it \"runs respond method\", :vcr do\n      related_docs = [{content: \"My name is Shohei\"}, {content: \"I'm a software engineer\"}]\n      broca = LlmMemory::Broca.new(prompt: template)\n      res = broca.respond(related_docs: related_docs, query_str: \"what is my name?\")\n      expect(res).to include(\"Shohei\")\n    end\n\n    it \"runs respond_with_schema method\", :vcr do\n      related_docs = [{content: \"My name is Shohei\"}, {content: \"I'm a software engineer\"}]\n      broca = LlmMemory::Broca.new(prompt: template)\n      res = broca.respond_with_schema(\n        context: {related_docs: related_docs, query_str: \"what is my name?\"},\n        schema: {\n          type: :object,\n          properties: {\n            name: {\n              type: :string,\n              description: \"The name of person\"\n            }\n          },\n          required: [\"name\"]\n        }\n      )\n      expect(res).to include({name: \"Shohei\"})\n    end\n  end\nend\n",
      "path": "/spec/llm_memory/"
    },
    {
      "filename": "wernicke_spec.rb",
      "content": "require \"spec_helper\"\nrequire \"llm_memory/wernicke\"\n\nRSpec.describe LlmMemory::Wernicke do\n  describe \"file loder\" do\n    it \"should returns docs\" do\n      directory_path = \"path/to/your/directory\"\n      file_path1 = \"path/to/your/directory/file1.txt\"\n      file_path2 = \"path/to/your/directory/file2.txt\"\n\n      file_content1 = \"This is file1 content.\"\n      file_content2 = \"This is file2 content.\"\n      now = Time.now\n      timestamp = now.strftime(\"%Y%m%d%H%M%S\")\n\n      # Stub Find.find and File.directory? methods\n      allow(Find).to receive(:find).with(directory_path).and_yield(file_path1).and_yield(file_path2)\n      allow(File).to receive(:directory?).and_return(false)\n\n      # Stub File.read method\n      allow(File).to receive(:read).with(file_path1).and_return(file_content1)\n      allow(File).to receive(:read).with(file_path2).and_return(file_content2)\n\n      allow(File).to receive(:ctime).with(file_path1).and_return(now)\n      allow(File).to receive(:ctime).with(file_path2).and_return(now)\n\n      docs = LlmMemory::Wernicke.load(:file, directory_path)\n      expect(docs).to eq(\n        [\n          {content: \"This is file1 content.\", metadata: {file_name: \"file1.txt\", timestamp: timestamp}},\n          {content: \"This is file2 content.\", metadata: {file_name: \"file2.txt\", timestamp: timestamp}}\n        ]\n      )\n    end\n  end\nend\n",
      "path": "/spec/llm_memory/"
    },
    {
      "filename": "hippocampus_spec.rb",
      "content": "require \"spec_helper\"\nrequire \"llm_memory/hippocampus\"\nrequire \"llm_memory_pgvector\"\n\nRSpec.describe LlmMemory::Hippocampus do\n  it \"instantiates a new Broca object\" do\n    hippocampus = LlmMemory::Hippocampus.new\n    expect(hippocampus).to be_a(LlmMemory::Hippocampus)\n  end\n\n  describe \"make_chunks\" do\n    it \"should returns the same docs if content length is less than chunk_size\" do\n      hippocampus = LlmMemory::Hippocampus.new\n      docs = [{content: \"foo bar\", metadata: {info: \"test\"}}]\n      docs = hippocampus.make_chunks(docs)\n      expect(docs).to eq([{content: \"foo bar\", metadata: {info: \"test\"}}])\n    end\n\n    it \"should returns chunked docs\" do\n      hippocampus = LlmMemory::Hippocampus.new(chunk_size: 4, chunk_overlap: 2)\n      docs = [{content: \"123456789\", metadata: {info: \"test\"}}]\n      docs = hippocampus.make_chunks(docs)\n      expect(docs).to eq([\n        {content: \"1234\", metadata: {info: \"test\"}},\n        {content: \"3456\", metadata: {info: \"test\"}},\n        {content: \"5678\", metadata: {info: \"test\"}},\n        {content: \"789\", metadata: {info: \"test\"}}\n      ])\n    end\n\n    it \"should returns chunked docs\" do\n      hippocampus = LlmMemory::Hippocampus.new(chunk_size: 5, chunk_overlap: 3)\n      docs = [{content: \"123456789\", metadata: {info: \"test\"}}]\n      docs = hippocampus.make_chunks(docs)\n      expect(docs).to eq([\n        {content: \"12345\", metadata: {info: \"test\"}},\n        {content: \"34567\", metadata: {info: \"test\"}},\n        {content: \"56789\", metadata: {info: \"test\"}}\n      ])\n    end\n  end\n\n  describe \"add_vectors\", :vcr do\n    it \"should returns docs that has vector\" do\n      hippocampus = LlmMemory::Hippocampus.new(chunk_size: 1, chunk_overlap: 0)\n      docs = [{content: \"Hello, I'm Shohei. I'm working as a sotware developer\", metadata: {info: \"test\"}}]\n      docs = hippocampus.add_vectors(docs)\n      doc = docs.first\n      expect(doc[:vector].length).to eq(1536)\n      expect(doc[:content]).to eq(\"Hello, I'm Shohei. I'm working as a sotware developer\")\n      expect(doc[:metadata]).to eq({info: \"test\"})\n    end\n  end\n\n  describe \"memorize\", :vcr do\n    it \"should add docs to redis\", :vcr do\n      hippocampus = LlmMemory::Hippocampus.new\n      timestamp = \"20201231235959\"\n      docs = [\n        {content: \"Hello, I'm Shohei.\", metadata: {info: \"name\", timestamp: timestamp}},\n        {content: \"I'm working as a sotware developer\", metadata: {info: \"profession\", timestamp: timestamp}},\n        {content: \"I like music\", metadata: {info: \"hobby\", timestamp: timestamp}}\n      ]\n      hippocampus.forget_all\n      res = hippocampus.memorize(docs)\n      expect(res.keys.map { |k| k.split(\":\")[2] }.uniq.first).to eq timestamp\n      expect(hippocampus.list.sort).to eq res.keys.sort\n      expect(res.values.first).to eq(\"Hello, I'm Shohei.\")\n      hippocampus.forget(res.keys.first)\n      expect(hippocampus.list(\"*:#{timestamp}:*\").size).to eq(2)\n    end\n  end\n\n  describe \"query\", :vcr do\n    it \"search from redis and find\", :vcr do\n      hippocampus = LlmMemory::Hippocampus.new\n      timestamp = \"20201231235959\"\n      docs = [\n        {content: \"Hello, I'm Shohei.\", metadata: {info: \"name\", timestamp: timestamp}},\n        {content: \"I'm working as a sotware developer\", metadata: {info: \"profession\", timestamp: timestamp}},\n        {content: \"I like music\", metadata: {info: \"hobby\", timestamp: timestamp}}\n      ]\n      hippocampus.forget_all\n      hippocampus.memorize(docs)\n      res = hippocampus.query(\"What is my name?\", limit: 2)\n      # pp res\n      expect(res.length).to eq(2)\n      expect(res.first[:content]).to eq(\"Hello, I'm Shohei.\")\n      expect(res.first[:metadata][:info]).to eq(\"name\")\n    end\n  end\n\n  describe \"pgvector\" do\n    before do\n      LlmMemoryPgvector.configure do |c|\n        c.pg_url = \"postgresql://postgres:foobar@localhost\"\n      end\n    end\n\n    it \"search from vgvector and find\", :vcr do\n      hippocampus = LlmMemory::Hippocampus.new(store: :pgvector)\n      hippocampus.forget_all\n      docs = [\n        {content: \"Hello, I'm Shohei.\", metadata: {info: \"name\"}},\n        {content: \"I'm working as a sotware developer\", metadata: {info: \"profession\"}},\n        {content: \"I like music\", metadata: {info: \"hobby\"}}\n      ]\n      hippocampus.memorize(docs)\n      expect(hippocampus.list.map { |s| s[\"id\"] }).to eq([1, 2, 3])\n      expect(hippocampus.list(id: [1, 2]).map { |s| s[\"id\"] }).to eq([1, 2])\n      res = hippocampus.query(\"What is my name?\", limit: 2)\n      expect(res.first[:content]).to eq(\"Hello, I'm Shohei.\")\n      expect(res.first[:metadata][:info]).to eq(\"name\")\n    end\n  end\nend\n",
      "path": "/spec/llm_memory/"
    },
    {
      "filename": "spec_helper.rb",
      "content": "# frozen_string_literal: true\n\nrequire \"support/vcr\"\nrequire \"llm_memory\"\n\nRSpec.configure do |config|\n  # Enable flags like --only-failures and --next-failure\n  config.example_status_persistence_file_path = \".rspec_status\"\n\n  # Disable RSpec exposing methods globally on `Module` and `main`\n  config.disable_monkey_patching!\n\n  config.expect_with :rspec do |c|\n    c.syntax = :expect\n  end\n\n  if ENV.fetch(\"OPENAI_ACCESS_TOKEN\", nil)\n    warning = \"WARNING! Specs are hitting the OpenAI API using your OPENAI_ACCESS_TOKEN! This\ncosts at least 2 cents per run and is very slow! If you don't want this, unset\nOPENAI_ACCESS_TOKEN to just run against the stored VCR responses.\"\n    warning = RSpec::Core::Formatters::ConsoleCodes.wrap(warning, :bold_red)\n\n    config.before(:suite) { RSpec.configuration.reporter.message(warning) }\n    config.after(:suite) { RSpec.configuration.reporter.message(warning) }\n  end\n\n  config.before(:all) do\n    LlmMemory.configure do |c|\n      c.openai_access_token = ENV.fetch(\"OPENAI_ACCESS_TOKEN\", \"dummy-token\")\n      c.redis_url = ENV.fetch(\"REDISCLOUD_URL\", \"redis://localhost:6379\")\n    end\n  end\nend\n",
      "path": "/spec/"
    },
    {
      "filename": "llm_memory_spec.rb",
      "content": "# frozen_string_literal: true\n\nRSpec.describe LlmMemory do\n  it \"has a version number\" do\n    expect(LlmMemory::VERSION).not_to be nil\n  end\nend\n",
      "path": "/spec/"
    },
    {
      "filename": "vcr.rb",
      "content": "require \"vcr\"\n\nVCR.configure do |config|\n  config.cassette_library_dir = \"spec/fixtures/vcr_cassettes\"\n  config.hook_into :webmock\n  config.configure_rspec_metadata!\n\n  config.default_cassette_options = {\n    record: ENV.fetch(\"OPENAI_ACCESS_TOKEN\", nil) ? :all : :new_episodes\n  }\n\n  config.filter_sensitive_data(\"<OPENAI_ACCESS_TOKEN>\") { ENV[\"OPENAI_ACCESS_TOKEN\"] }\n  config.filter_sensitive_data(\"<OPENAI_ORGANIZATION_ID>\") { ENV[\"OPENAI_ORGANIZATION_ID\"] }\n\n  # config.debug_logger = $stdout\n\n  config.ignore_request do |request|\n    request.uri == \"https://huggingface.co/gpt2/resolve/main/tokenizer.json\"\n  end\n\n  # Optionally, you can filter sensitive data, such as API keys, from the recorded cassettes.\n  # config.filter_sensitive_data('<API_KEY>') { ENV['API_KEY'] }\nend\n",
      "path": "/spec/support/"
    }
  ]
}
